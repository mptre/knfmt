DESIGN
======

I've always wanted a tool capable of formatting source code according to
style(9) from OpenBSD. It shouldn't require one to fiddle with a bunch of knobs
nor supply needed include path(s) in order to do get the desired formatting.
There are other tools out there but they didn't manage to satisfy my criteria,
i.e. not invented here.

Another motivation for knfmt was the discovery of a paper entitled "A prettier
printer" by Philip Wadler[1]. I found his work both novel and elegant. This was
a chance to put his ideas into use, which others already have done[2].

This document is an attempt to give an introducing to the knfmt source code and
is divided one section per file. The same files are enumerated in order of
execution. Followed by some noteworthy topics in no particular order.

[1] https://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf
[2] http://blog.vjeux.com/2017/javascript/anatomy-of-a-javascript-pretty-printer.html

lexer.c
=======

The lexer turns source code into a list of tokens. It does so by reading the
source code into memory and then constructs the list of tokens. The lexer owns
the memory associated with the list of tokens. This has several advantages as
all tokens are therefore constant and pointers to them remains valid until
lexer_free() is invoked.

In general terms, the lexer API is divided into two categories:

1. Routines that consume one or many tokens, often prefixed if lexer_if.
   Such routines returns non-zero if it managed to consume the next token(s)
   given a criteria, such as being of a certain type. Otherwise, zero is
   returned.

2. Routines that peek at one or many tokens, prefixed with lexer_peek. Such
   routines returns non-zero if the peeked at token(s) satisfied the given
   criteria, such as being of a certain type. Otherwise, zero is returned.

A token can have more tokens associated with it, called dangling tokens. See cpp
below for such use case.

parser.c
========

The parser recognizes the list of tokens emitted by the lexer according to the C
grammar, in a recursive descent fashion. The parser is also responsible for
constructing the document which represents the formatted source code. Parsing is
therefore not implemented as a distinct step of the execution. It might be
beneficial to separate the two at some point by letting the parser only be
concerned with creation of an AST like structure.

expr.c
======

The expr parser is implemented using the Pratt parsing technique[1], a common
approach[2][3].

Some expressions which are valid C are not recognized the parser without
assistance from the parser. One example is sizeof() which can be given a type.
Whenever the parser encounters something it cannot recognize it gives the parser
a chance to recover, allowing the parser to continue; see
parser_exec_expr_recover() and cpp below.

[1] https://en.wikipedia.org/wiki/Operator-precedence_parser#Pratt_parsing
[2] https://craftinginterpreters.com/compiling-expressions.html
[3] https://quasilyte.dev/blog/post/pratt-parsers-go/

doc.c
=====

A document represents the formatted source code. In its essence, a document
consists of different types of documents forming a tree which roughly
reassembles the structure of the source code. The most important type of
document is the group which represents something that's intended to fit on a
single line. In doc_exec(), the document tree is traversed causing the formatted
source code to be emitted. Upon entering a group, it checks if all documents
nested underneath it fits on the current line, see doc_fits(). If not, break
mode is entered in which line breaks are allowed to be emitted in the hopes of
not crossing the maximum number of columns per line. If it does fit, munge mode
is entered in which no line breaks are emitted. Causing the document
to reconsider switching between the two modes can therefore only be achieved by
entering a group.

The document representation and any decision to switch between the two modes can
be examined by invoking knfmt as follows:

	$ knfmt -vv

The following type of documents are available:

* DOC_CONCAT
  Concat consists of zero or more nested documents. It's often used as a child
  of a group document.

* DOC_GROUP
  Group represents something that's intended to fit on a single line. This is
  the only type of document that can trigger a refit.

* DOC_INDENT
  Indent increases the indentation, the same indentation is not emitted until
  after emitting a new line.

* DOC_DEDENT
  Dedent immediately decreases the current indentation by trimming the current
  line, assuming nothing other than indentation has been emitted on the current
  line. One use case for this type of document is goto labels which should never
  be indented, effectively disregarding the current indentation completely.

* DOC_ALIGN
  Align emits enough white space in order to reach the column associated with
  the same document. Used the by the ruler.

* DOC_LITERAL
  Literal represents a string that must be emitted as is. Tokens emitted by the
  lexer are turned into literal documents.

* DOC_VERBATIM
  Verbatim represents parts of the source code taken as is, such as comments and
  preprocessor directives. These type of documents often carry a trailing new
  line which requires some special care.

* DOC_LINE
  Line emits a new line while in break mode and a space in munge mode.

* DOC_SOFTLINE
  Softline emits a new line while in break mode and nothing in munge mode.

* DOC_HARDLINE
  Hardline emits a new line in both break and munge mode.

* DOC_NOLINE
  Noline prevents any type of line document nested underneath it from being
  emitted.

* DOC_MUTE
  Mute informs doc_print() to not emit anything. Used while traversing cpp
  conditionals.

ruler.c
=======

The ruler aligns certain language constructs such as function prototypes and
variable declarations. The parser is responsible for calling ruler_insert() on
each line it wants to align. Note it can also call ruler_insert() for many
columns on a single line. Once all lines that must be aligned are emitted the
parser calls ruler_exec() which calculates the needed alignment for each line
and column. The ruler is at this point aware of all columns and can therefore
deduce the longest one calculate the necessary alignment accordingly.

cpp
===

Recall, knfmt does not run the source code through the preprocessor since it
would require one to supply the required include path(s) which in my opinion
makes the utility less user friendly. That has one severe downside of not being
able to distinguish preprocessor directives from other well defined language
constructs.

Usage of preprocessor directives can literally be present anywhere in the source
code. Letting the parser cope with this fact would make the implementation
tedious. Instead, preprocessor directives are made completely transparent to the
parser. An emitted token by the lexer will never directly represent a
preprocessor directive. Instead, a token always represents a well defined
language construct and can instead have one or many dangling tokens associated
with it, see tk_prefixes and tk_suffixes. Such dangling tokens can among other
things represent preprocessor directives. The dangling tokens are emitted by
doc_token().

One particular type of preprocessor macros that requires special attention are
the ones that expands to a loop construct. Such macros often include the word
foreach in the identifier. Not being aware that such macros actually expand to
a loop construct would otherwise make the parser halt as something that looks
like a function call followed by either a pair of braces or another statement is
not considered valid C, see parser_exec_stmt(). An example of such macros are
the ones provided by queue(3).

Another problematic type of preprocessor macros are the ones that expands to a
binary expression. Such macros often have a binary operator as a distinct
argument. Macro invocations are handled by expr.c as they look like function
calls without any further preprocessor knowledge. Encountering a binary operator
in an unary context causes the expr parser to halt. However, in such scenarios
it gives the parser a chance to recover from the situation; allowing the expr
parser to continue, see parser_exec_expr_recover(). An example of such macros
are the ones provided by timercmp(3).

comments
========

Comments are never emitted directly by lexer but are instead represented as
dangling tokens, see cpp above.
